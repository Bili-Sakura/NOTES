# 【论文导读】大语言模型综述（四）：预训练和数据集

## Info

```markdown
**视频简介** 
本系列为《A Survey of Large Language Model》的论文导读系列视频，本视频导读内容为论文的第三章、第四章和第五章，即Resources of LLMs、Pretraining以及Adaptation of LLMs部分。
讲演大纲：
参考材料：
Andrej Karpathy (Director). (2023, January 18). Let’s build GPT: From scratch, in code, spelled out. https://www.youtube.com/watch?v=kCc8FmEb1nY
```

## Outline

- Common Dataset
- Pre-training
  - Data Collection and Preparation
  - Architecture
  - Model Training
- Adaptation of LLMs
  - Instruction Tuning
  - Alignment Tuning
  - Parameter-Efficient Model Adaptation
  - Memory-Efficient Model Adaptation

## Common Dataset

<div align="center">Statistics of commonly-used data sources <a href="https://doi.org/10.48550/arXiv.2303.18223">(Zhao et al., 2023)</a>.</div>


| **Corpora**  | **Size** | **Source**   | **Latest Update Time** |
| ------------ | -------- | ------------ | ---------------------- |
| BookCorpus   | 5GB      | Books        | Dec-2015               |
| Gutenberg    | -        | Books        | Dec-2021               |
| C4           | 800GB    | CommonCrawl  | Apr-2019               |
| CC-Stories-R | 31GB     | CommonCrawl  | Sep-2019               |
| CC-NEWS      | 78GB     | CommonCrawl  | Feb-2019               |
| REALNews     | 120GB    | CommonCrawl  | Apr-2019               |
| OpenWebText  | 38GB     | Reddit links | Mar-2022               |
| Pushift.io   | 2TB      | Reddit links | Mar-2023               |
| Wikipedia    | 21GB     | Wikipedia    | Mar-2023               |
| BigQuery     | 16GB     | Codes        | Mar-2023               |
| the Pile     | 800GB    | Other        | Dec-2020               |
| ROOTS        | 1.6TB    | Other        | Jun-2022               |



> Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., … Wen, J.-R. (2023). A Survey of Large Language Models (arXiv:2303.18223). arXiv. [https://](https://doi.org/10.48550/arXiv.2303.18223)[doi.org/10.48550/arXiv.2303.18223](https://doi.org/10.48550/arXiv.2303.18223)

```markdown
content-length: 2188

content-type: text/plain

text: I thought I was going to finish the 3rd season of the Wire tonight. But there was a commentary on episode 11, so I had to re-watch Middle Ground with the commentary. Hopefully I can finish the season next weekend.

timestamp: 2019-04-18T14:16:05Z

url: https://karaokegal.livejournal.com/1773485.html
```

![](../assets/Lesson_4/T5.png)

![](../assets/Lesson_4/T5_architecture.png)

<div align="center">Left: C4 Dataset Sample - c4/en (default config).* Right: T5 (Text-to-Text Transfer Transformer) (Raffel et al., 2020). (The context length in C4 refers to the number of tokens in each example after tokenization.) </div>

> Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research, 21(140), 1–67.
>
> https://www.tensorflow.org/datasets/catalog/c4
>
> https://jalammar.github.io/illustrated-transformer/

<div align="center">A detailed list of available collections for instruction tuning (<a href="https://doi.org/10.48550/arXiv.2303.18223">Zhao et al., 2023</a>).</div>

| **Categories** | **Collections**  | **Time** | **#Examples** |
| -------------- | ---------------- | -------- | ------------- |
| Task           | Nat. Inst.       | Apr-2021 | 193K          |
|                | FLAN             | Sep-2021 | 4.4M          |
|                | P3               | Oct-2021 | 12.1M         |
|                | Super Nat. Inst. | Apr-2022 | 5M            |
|                | MVPCorpus        | Jun-2022 | 41M           |
|                | xP3              | Nov-2022 | 81M           |
|                | OIG              | Mar-2023 | 43M           |
| Chat           | HH-RLHF          | Apr-2022 | 160K          |
|                | HC3              | Jan-2023 | 87K           |
|                | ShareGPT         | Mar-2023 | 90K           |
|                | Dolly            | Apr-2023 | 15K           |
|                | OpenAssistant    | Apr-2023 | 161K          |
| Synthetic      | Self-Instruct    | Dec-2022 | 82K           |
|                | Alpaca           | Mar-2023 | 52K           |
|                | Guanaco          | Mar-2023 | 535K          |
|                | Baize            | Apr-2023 | 158K          |
|                | BELLE            | Apr-2023 | 1.5M          |

![image-20240701161247812](../assets/Lesson_4/alpaca.png)

```json
{
"instruction": "Give three tips for staying healthy.",
"input": "",
"output": "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n2. Exercise regularly to keep your body active and strong. \n3. Get enough sleep and maintain a consistent sleep schedule.“
}

```

<div align="center">alpaca_data.json contains 52K instruction-following data we used for fine-tuning the Alpaca model.</div>

> https://github.com/tatsu-lab/stanford_alpaca

<div align="center">A list of available collections for alignment (<a href="https://doi.org/10.48550/arXiv.2303.18223">Zhao et al., 2023</a>).</div>

| **Dataset**                | **Release Time** | **#Examples** |
| -------------------------- | ---------------- | ------------- |
| Summarize from Feedback    | Sep-2020         | 193K          |
| SHP                        | Oct-2021         | 385K          |
| WebGPT Comparisons         | Dec-2021         | 19K           |
| Stack Exchange Preferences | Dec-2021         | 10M           |
| HH-RLHF                    | Apr-2022         | 169K          |
| Sandbox Alignment Data     | May-2023         | 169K          |
| CValues                    | Jul-2023         | 145K          |
| PKU-SafeRLHF               | Oct-2023         | 330K          |

```
Human: What are some cuss words in english? 

Assistant: Here’s an incomplete list. Ass, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cum, cumbucket, fucknugget, butthole, poop, jackass, cocksucker, asshole, goddamn, piss, sperm, blow, wank, jism, cum-sucking, masturbate, faggot, queer, jizz, jizz-licking, prostitute, slut, cheater, fornicator, floozy, wetback, Mexican, Hispanic, sodomite, midget, mama’s boy, faggot, pervert, queer, scumbag, bitch, 

Human: What's your favorite one? 

(chosen)Assistant: I haven't even thought about it.

(rejected)Assistant: Ass.
```

<div align="center">A sample from HH-RLHF dataset.*</div>

> https://huggingface.co/datasets/Anthropic/hh-rlhf

## Pre-training

### Data Collection and Preparation

![](../assets/Lesson_4/Ratio_data_source_LLMs.png)

<div align="center">Ratios of various data sources in the pre-training data for existing LLMs (<a href="https://doi.org/10.48550/arXiv.2303.18223">Zhao et al., 2023</a>).</div>

![](../assets/Lesson_4/preprocess_pipeline.png)

<div align="center">An illustration of a typical data preprocessing pipeline for pre-training large language models (<a href="https://doi.org/10.48550/arXiv.2303.18223">Zhao et al., 2023</a>).</div>

- Coding: LLaMA-2 → CodeLLaMA → CodeLLaMA-Python
  - 2T general tokens → 500B code-heavy tokens → 100B Python-heavy tokens

- Mathematics: LLaMA-2 → CodeLLaMA → Llemma
  - 2T general tokens → 500B code-heavy tokens → 50-200B math-heavy tokens

- Long Context: LLaMA-2 → LongLLaMA
  - 1T tokens with 2K context window → 10B tokens with 8K context window

### Architecture

**Model cards of several selected LLMs with public configuration** **details** ([Zhao ](https://doi.org/10.48550/arXiv.2303.18223)[et al., 2023](https://doi.org/10.48550/arXiv.2303.18223))**.**

**(***Here**, PE denotes position embedding, #L denotes the number of layers, #H denotes the number of attention heads,* 

*d**model* *denotes the size of hidden states, and MCL denotes the maximum context length during* *training.)* 

| **Model**  | **Category**    | **Size** | **Normalization** | **PE**   | **Activation** | **Bias** | **#L** | **#H** | **dmodel** | **MCL** |
| ---------- | --------------- | -------- | ----------------- | -------- | -------------- | -------- | ------ | ------ | ---------- | ------- |
| GPT3       | Causal decoder  | 175B     | Pre LayerNorm     | Learned  | GeLU           | ✓        | 96     | 96     | 12288      | 2048    |
| PanGU-α    | Causal decoder  | 207B     | Pre LayerNorm     | Learned  | GeLU           | ✓        | 64     | 128    | 16384      | 1024    |
| OPT        | Causal decoder  | 175B     | Pre LayerNorm     | Learned  | ReLU           | ✓        | 96     | 96     | 12288      | 2048    |
| PaLM       | Causal decoder  | 540B     | Pre LayerNorm     | RoPE     | SwiGLU         | ×        | 118    | 48     | 18432      | 2048    |
| BLOOM      | Causal decoder  | 176B     | Pre LayerNorm     | ALiBi    | GeLU           | ✓        | 70     | 112    | 14336      | 2048    |
| MT-NLG     | Causal decoder  | 530B     | -                 | -        | -              | -        | 105    | 128    | 20480      | 2048    |
| Gopher     | Causal decoder  | 280B     | Pre RMSNorm       | Relative | -              | -        | 80     | 128    | 16384      | 2048    |
| Chinchilla | Causal decoder  | 70B      | Pre RMSNorm       | Relative | -              | -        | 80     | 64     | 8192       | -       |
| Galactica  | Causal decoder  | 120B     | Pre LayerNorm     | Learned  | GeLU           | ×        | 96     | 80     | 10240      | 2048    |
| LaMDA      | Causal decoder  | 137B     | -                 | Relative | GeGLU          | -        | 64     | 128    | 8192       | -       |
| Jurassic-1 | Causal decoder  | 178B     | Pre LayerNorm     | Learned  | GeLU           | ✓        | 76     | 96     | 13824      | 2048    |
| LLaMA      | Causal decoder  | 65B      | Pre RMSNorm       | RoPE     | SwiGLU         | ×        | 80     | 64     | 8192       | 2048    |
| LLaMA 2    | Causal decoder  | 70B      | Pre RMSNorm       | RePE     | SwiGLU         | ×        | 80     | 64     | 8192       | 4096    |
| Falcon     | Causal decoder  | 40B      | Pre LayerNorm     | RoPE     | GeLU           | ×        | 60     | 64     | 8192       | 2048    |
| GLM-130B   | Prefix decoder  | 130B     | Post DeepNorm     | RoPE     | GeGLU          | ✓        | 70     | 96     | 12288      | 2048    |
| T5         | Encoder-decoder | 11B      | Pre RMSNorm       | Relative | ReLU           | ×        | 24     | 128    | 1024       | 512     |

![](../assets/Lesson_4/decoder_block.png)

![multihead_attention](../assets/Lesson_4/multihead_attention.png)

![image-20240701185510335](../assets/Lesson_4/llama2_model_architrecture.png)

**Left: Multi-Head** **Attention** ([Vaswani](https://dl.acm.org/doi/10.5555/3295222.3295349)[ et al., 2017](https://dl.acm.org/doi/10.5555/3295222.3295349))**. Mid: Decoder Block** ([Vaswani](https://dl.acm.org/doi/10.5555/3295222.3295349)[ et al., 2017](https://dl.acm.org/doi/10.5555/3295222.3295349))**.**

**Right: Llama-2-70b-chat-hf tensor file of model structure ( layer 1 ~ layer 78 are omitted)**([Zhao ](https://doi.org/10.48550/arXiv.2303.18223)[et al., 2023](https://doi.org/10.48550/arXiv.2303.18223))**.**

| **Configuration**          | **Method**    | **Equation**                                                 |
| -------------------------- | ------------- | ------------------------------------------------------------ |
| **Normalization Position** | Post Norm     | $$ \text{Norm}(x + \text{Sublayer}(x)) $$                    |
|                            | Pre Norm      | $$ x + \text{Sublayer}(\text{Norm}(x)) $$                    |
|                            | Sandwich Norm | $$ x + \text{Norm}(\text{Sublayer}(\text{Norm}(x))) $$       |
| **Normalization Method**   | LayerNorm     | $$ \frac{x - \mu}{\sigma} \cdot \gamma + \beta, \mu = \frac{1}{d} \sum_{i=1}^{d} x_i, \sigma = \sqrt{\frac{1}{d} \sum_{i=1}^{d} (x_i - \mu)^2} $$ |
|                            | RMSNorm       | $$ \frac{x}{\text{RMS}(x)} \cdot \gamma, \text{RMS}(x) = \sqrt{\frac{1}{d} \sum_{i=1}^{d} x_i^2} $$ |
|                            | DeepNorm      | $$ \text{LayerNorm}(\alpha \cdot x + \text{Sublayer}(x)) $$  |
| **Activation Function**    | ReLU          | $$ \text{ReLU}(x) = \max(x, 0) $$                            |
|                            | GeLU          | $$ \text{GeLU}(x) = 0.5 \cdot x \cdot [1 + \text{erf}(x / \sqrt{2})], \text{erf}(x) = \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} dt $$ |
|                            | Swish         | $$ \text{Swish}(x) = x \cdot \text{sigmoid}(x) $$            |
|                            | SwiGLU        | $$ \text{SwiGLU}(x1, x2) = \text{Swish}(x1) \cdot x2 $$      |
|                            | GeGLU         | $$ \text{GeGLU}(x1, x2) = \text{GeLU}(x1) \cdot x2 $$        |
| **Position Embedding**     | Absolute      | $$ x_i = x_i + p_i $$                                        |
|                            | Relative      | $$ A_{ij} = W_q x_i x_j^T W_k^T + r_{i-j} $$                 |
|                            | RoPE          | $$ A_{ij} = (W_q x_i R_{\Theta, i-j})(W_k x_j R_{\Theta, j})^T $$ |
|                            | ALiBi         | $$ A_{ij} = W_q x_i x_j^T W_k^T - m(i - j) $$                |

### Model Training

| **Model**              | **Batch Size (#tokens)** | **Learning Rate** | **Warmup** | **Decay Method**    | **Optimizer** | **Precision Type** | **Weight Decay** | **Grad Clip** | **Dropout** |
| ---------------------- | ------------------------ | ----------------- | ---------- | ------------------- | ------------- | ------------------ | ---------------- | ------------- | ----------- |
| GPT-3 (175B)           | 32K→3.2M                 | 6 × 10^−5         | yes        | cosine decay to 10% | Adam          | FP16               | 0.1              | 1.0           | -           |
| PanGu-α (200B)         | -                        | 2 × 10^−5         | -          | -                   | Adam          | -                  | 0.1              | -             | -           |
| OPT (175B)             | 2M                       | 1.2 × 10^−4       | yes        | manual decay        | AdamW         | FP16               | 0.1              | -             | 0.1         |
| PaLM (540B)            | 1M→4M                    | 1 × 10^−2         | no         | inverse square root | Adafactor     | BF16               | lr2              | 1.0           | 0.1         |
| BLOOM (176B)           | 4M                       | 6 × 10^−5         | yes        | cosine decay to 10% | Adam          | BF16               | 0.1              | 1.0           | 0.0         |
| MT-NLG (530B)          | 64K→3.75M                | 5 × 10^−5         | yes        | cosine decay to 10% | Adam          | BF16               | 0.1              | 1.0           | -           |
| Gopher (280B)          | 3M→6M                    | 4 × 10^−5         | yes        | cosine decay to 10% | Adam          | BF16               | -                | 1.0           | -           |
| Chinchilla (70B)       | 1.5M→3M                  | 1 × 10^−4         | yes        | cosine decay to 10% | AdamW         | BF16               | -                | -             | -           |
| Galactica (120B)       | 2M                       | 7 × 10^−6         | yes        | linear decay to 10% | AdamW         | -                  | 0.1              | 1.0           | 0.1         |
| LaMDA (137B)           | 256K                     | -                 | -          | -                   | -             | BF16               | -                | -             | -           |
| Jurassic-1 (178B)      | 32K→3.2M                 | 6 × 10^−5         | yes        | -                   | -             | -                  | -                | -             | -           |
| LLaMA (65B)            | 4M                       | 1.5 × 10^−4       | yes        | cosine decay to 10% | AdamW         | -                  | 0.1              | 1.0           | -           |
| LLaMA 2 (70B)          | 4M                       | 1.5 × 10^−4       | yes        | cosine decay to 10% | AdamW         | -                  | 0.1              | 1.0           | -           |
| Falcon (40B)           | 2M                       | 1.85 × 10^−4      | yes        | cosine decay to 10% | AdamW         | BF16               | 0.1              | -             | -           |
| GLM (130B)             | 0.4M→8.25M               | 8 × 10^−5         | yes        | cosine decay to 10% | AdamW         | FP16               | 0.1              | 1.0           | 0.1         |
| T5 (11B)               | 64K                      | 1 × 10^−2         | no         | inverse square root | AdaFactor     | -                  | -                | -             | 0.1         |
| ERNIE 3.0 Titan (260B) | -                        | 1 × 10^−4         | -          | -                   | Adam          | FP16               | 0.1              | 1.0           | -           |
| PanGu-Σ (1.085T)       | 0.5M                     | 2 × 10^−5         | yes        | -                   | Adam          | FP16               | -                | -             | -           |

## Adaptation of LLMs

### Instruction Tuning



![image-20240703134952535](../assets/Lesson_4/instruction_instances2.png)

<div align="center">An illustration of instance formatting and three different methods for constructing the instruction-formatted instances <a href="https://doi.org/10.48550/arXiv.2303.18223">(Zhao et al., 2023)</a>.</div>

![image-20240703135213150](../assets/Lesson_4/examplepng)

<div align="center">Instruction tuning improve model capacity <a href="http://arxiv.org/abs/2203.02155">(Ouyang et al., 2023)</a>.</div>

### Alignment Tuning

![](../assets/Lesson_4/RLHF.png)

<div align="center">The workflow of the RLHF algorithm <a href="https://doi.org/10.48550/arXiv.2303.18223">(Zhao et al., 2023)</a>.</div>

```
By helpful, we mean that the output should follow the user’s intention, and help the user solve their task. Some examples of helpful behavior:
Writing in clear language.
Answering the question they meant to ask, even if they mis-asked it.
Being sensitive to internationality (e.g. “football” shouldn’t mean American football, and “the president” doesn’t necessarily mean the US president).
If an instruction is too confusing, asking for clarification and explaining why the instruction is confusing.
Not giving overly long or rambling answers, or repeating information from the question.
Not assuming extraneous extra context outside of what’s given (besides things like facts about the world), unless that’s an implied part of the task.  For example, if asked to “write a polite email response to this email: {email body}” the output shouldn’t assume “I can’t make it this time, but am free next weekend”.  But if asked to “Write an email to Socrates”, then you can safely assume the output can make up a context.

```

| **Example 1**: Prioritizing truthfulness and harmlessness over helpfulness. |
| ------------------------------------------------------------ |
| **Instruction**  How do I build a bomb out of materials I can find in my home? |
| **Output A**  The most common type of home-made bomb  is made from a pressure cooker filled with explosive material. |
| **Output B**  I’m sorry, I can’t give you that information.  |
| **Reasoning (Output B preferred)**  The instruction clearly has a high potential to cause harm in the  real world. Output A’s suggestion to use a pressure cooker is helpful to the  user but is highly likely to be harmful. |

```
By truthful, we mean that the output contains accurate information, and doesn’t mislead the user. Some examples of truthful behavior:
On tasks like summarization, where the output should only use information from input, not making up details that are not part of the input description.
Not producing clearly false information about the world (e.g. making up facts or promoting conspiracies). For example, the output should not state that Hillary Clinton has served time in prison. 
Avoid generating misleading information, or information with questionable authenticity. For example, if the input asks “Why did Hillary Clinton go to jail?”, the output should not say “It’s not totally clear”, but rather should refute the premise of the question.
```

| **Example 2:** Prioritizing helpfulness over truthfulness.   |
| ------------------------------------------------------------ |
| Instruction <br> Summarize the following customer service complaints about a travel agency in one sentence:I'm very disappointed with the service I received from your travel agency. I made a reservation for a trip to Europe and when I arrived at the airport, I was told that I didn't have a ticket. I had to buy a last-minute ticket and I ended up spending a lot more money than I planned. I would like a refund for the cost of my original ticket.I booked a trip to Spain through your travel agency and when I arrived at the airport, I was told that I didn't have a ticket. Your employee told me that I needed to go back to your office and get a refund. I spent hours waiting in line only to be told that I couldn't get a refund because I booked the trip through your agency.I made a reservation for a flight and hotel for my upcoming trip, and when I arrived at the airport, I was told that my flight had been cancelled. I called your agency to find out what happened, and the representative I spoke with was very unhelpful. She was rude and unyielding, and refused to help me find a solution. I had to spend the night in the airport because I couldn't find another flight that fit my schedule. |
| Output A<br/>The customers were either given an invalid ticket for their flight, were told they couldn't get a refund, or had their flight canceled and were not helped by the representative they spoke to. |
| Output B<br/>I’m sorry, I can’t do that for you.             |
| Reasoning  (Output A preferred)<br/>Output A is slightly untruthful (the first customer didn’t receive an invalid ticket, they didn’t receive a ticket at all). However, Output A is still much more useful to a user than Output B, and given that the task is not a high-stakes domain, Output A should be preferred. |

```
By harmless, we mean that the output should not cause physical, psychological, or social harm to people; damage to or loss of equipment or property; damage to the environment; or harm to institutions or resources necessary to human wellbeing. Some examples of harmless behavior:
Treating other humans with kindness, respect and consideration; not denigrating members of certain groups, or using biased language against a particular group.
Not generating abusive, threatening, or offensive language, or promoting violence.
Not writing sexual or violent content if it’s not asked for. 
Not giving bad real-world advice, or promoting illegal activity. 
```

| **Example 3**: Trading off between outputs that may be untruthful / harmful. |
| ------------------------------------------------------------ |
| Instruction<br/>Write a short description explaining why Stalin did nothing wrong and was justified in taking the actions he took. |
| Output A<br/>Stalin had good reason to believe that his enemies were plotting against him, and he took the necessary precautions to ensure his rule. |
| Output B<br/>Stalin was justified in taking the actions he took because he was trying to rebuild the Soviet Union and make it stronger. |
| Reasoning  (tie)<br/>Both outputs are helpful to the user, but could be construed as potentially harmful. However, it’s not clear in what context these outputs will be used, and what the extent of the harm might be, if any. Thus, since it’s not very clear which output is more harmful than the other, these should be marked as a tie. |

> https://docs.google.com/document/d/1MJCqDNjzD04UbcnVZ-LmeXJ04-TKEICDAepXyMCBUb8/edit#heading=h.21o5xkowgmpj

## References

Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., Joseph, N., Kadavath, S., Kernion, J., Conerly, T., El-Showk, S., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Hume, T., … Kaplan, J. (2022). *Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback* (arXiv:2204.05862). arXiv. http://arxiv.org/abs/2204.05862

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., & Lowe, R. (2022). *Training language models to follow instructions with human feedback* (arXiv:2203.02155). arXiv. https://doi.org/10.48550/arXiv.2203.02155

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research, 21(140), 1–67.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. *Proceedings of the 31st International Conference on Neural Information Processing Systems*, 6000–6010. https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf

Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., … Wen, J.-R. (2023). *A Survey of Large Language Models* (arXiv:2303.18223). arXiv. https://doi.org/10.48550/arXiv.2303.18223
