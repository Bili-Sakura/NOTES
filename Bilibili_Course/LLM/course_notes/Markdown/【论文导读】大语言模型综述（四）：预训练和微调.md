# 【论文导读】大语言模型综述（四）：预训练和数据集

## Info

```markdown
**视频简介** 
本系列为《A Survey of Large Language Model》的论文导读系列视频，本视频导读内容为论文的第三章、第四章和第五章，即Resources of LLMs、Pretraining以及Adaptation of LLMs部分。
讲演大纲：
参考材料：
Andrej Karpathy (Director). (2023, January 18). Let’s build GPT: From scratch, in code, spelled out. https://www.youtube.com/watch?v=kCc8FmEb1nY
```

## Outline

- Common Dataset
- Pre-training
  - Data Collection and Preparation
  - Architecture
  - Model Training
- Adaptation of LLMs
  - Instruction Tuning
  - Alignment Tuning
  - Parameter-Efficient Model Adaptation
  - Memory-Efficient Model Adaptation

## Common Dataset

<div align="center">Statistics of commonly-used data sources <a href="https://doi.org/10.48550/arXiv.2303.18223">(Zhao et al., 2023)</a>.</div>


| **Corpora**  | **Size** | **Source**   | **Latest Update Time** |
| ------------ | -------- | ------------ | ---------------------- |
| BookCorpus   | 5GB      | Books        | Dec-2015               |
| Gutenberg    | -        | Books        | Dec-2021               |
| C4           | 800GB    | CommonCrawl  | Apr-2019               |
| CC-Stories-R | 31GB     | CommonCrawl  | Sep-2019               |
| CC-NEWS      | 78GB     | CommonCrawl  | Feb-2019               |
| REALNews     | 120GB    | CommonCrawl  | Apr-2019               |
| OpenWebText  | 38GB     | Reddit links | Mar-2022               |
| Pushift.io   | 2TB      | Reddit links | Mar-2023               |
| Wikipedia    | 21GB     | Wikipedia    | Mar-2023               |
| BigQuery     | 16GB     | Codes        | Mar-2023               |
| the Pile     | 800GB    | Other        | Dec-2020               |
| ROOTS        | 1.6TB    | Other        | Jun-2022               |



> Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., … Wen, J.-R. (2023). A Survey of Large Language Models (arXiv:2303.18223). arXiv. [https://](https://doi.org/10.48550/arXiv.2303.18223)[doi.org/10.48550/arXiv.2303.18223](https://doi.org/10.48550/arXiv.2303.18223)

```markdown
content-length: 2188

content-type: text/plain

text: I thought I was going to finish the 3rd season of the Wire tonight. But there was a commentary on episode 11, so I had to re-watch Middle Ground with the commentary. Hopefully I can finish the season next weekend.

timestamp: 2019-04-18T14:16:05Z

url: https://karaokegal.livejournal.com/1773485.html
```

![](../assets/Lesson_4/T5.png)

<div align="center">Left: C4 Dataset Sample - c4/en (default config).* Right: T5 (Text-to-Text Transfer Transformer) (Raffel et al., 2020). (The context length in C4 refers to the number of tokens in each example after tokenization.) </div>

> Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research, 21(140), 1–67.
>
> https://www.tensorflow.org/datasets/catalog/c4
> 

<div align="center">A detailed list of available collections for instruction tuning (<a href="https://doi.org/10.48550/arXiv.2303.18223">Zhao et al., 2023</a>).</div>

| **Categories** | **Collections**  | **Time** | **#Examples** |
| -------------- | ---------------- | -------- | ------------- |
| Task           | Nat. Inst.       | Apr-2021 | 193K          |
|                | FLAN             | Sep-2021 | 4.4M          |
|                | P3               | Oct-2021 | 12.1M         |
|                | Super Nat. Inst. | Apr-2022 | 5M            |
|                | MVPCorpus        | Jun-2022 | 41M           |
|                | xP3              | Nov-2022 | 81M           |
|                | OIG              | Mar-2023 | 43M           |
| Chat           | HH-RLHF          | Apr-2022 | 160K          |
|                | HC3              | Jan-2023 | 87K           |
|                | ShareGPT         | Mar-2023 | 90K           |
|                | Dolly            | Apr-2023 | 15K           |
|                | OpenAssistant    | Apr-2023 | 161K          |
| Synthetic      | Self-Instruct    | Dec-2022 | 82K           |
|                | Alpaca           | Mar-2023 | 52K           |
|                | Guanaco          | Mar-2023 | 535K          |
|                | Baize            | Apr-2023 | 158K          |
|                | BELLE            | Apr-2023 | 1.5M          |

![image-20240701161247812](../assets/Lesson_4/alpaca.png)

```json
{
"instruction": "Give three tips for staying healthy.",
"input": "",
"output": "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n2. Exercise regularly to keep your body active and strong. \n3. Get enough sleep and maintain a consistent sleep schedule.“
}

```

<div align="center">alpaca_data.json contains 52K instruction-following data we used for fine-tuning the Alpaca model.</div>

> https://github.com/tatsu-lab/stanford_alpaca

<div align="center">A list of available collections for alignment (<a href="https://doi.org/10.48550/arXiv.2303.18223">Zhao et al., 2023</a>).</div>

| **Dataset**                | **Release Time** | **#Examples** |
| -------------------------- | ---------------- | ------------- |
| Summarize from Feedback    | Sep-2020         | 193K          |
| SHP                        | Oct-2021         | 385K          |
| WebGPT Comparisons         | Dec-2021         | 19K           |
| Stack Exchange Preferences | Dec-2021         | 10M           |
| HH-RLHF                    | Apr-2022         | 169K          |
| Sandbox Alignment Data     | May-2023         | 169K          |
| CValues                    | Jul-2023         | 145K          |
| PKU-SafeRLHF               | Oct-2023         | 330K          |

```
Human: What are some cuss words in english? 

Assistant: Here’s an incomplete list. Ass, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cum, cumbucket, fucknugget, butthole, poop, jackass, cocksucker, asshole, goddamn, piss, sperm, blow, wank, jism, cum-sucking, masturbate, faggot, queer, jizz, jizz-licking, prostitute, slut, cheater, fornicator, floozy, wetback, Mexican, Hispanic, sodomite, midget, mama’s boy, faggot, pervert, queer, scumbag, bitch, 

Human: What's your favorite one? 

(chosen)Assistant: I haven't even thought about it.

(rejected)Assistant: Ass.
```

<div align="center">A sample from HH-RLHF dataset.*</div>

> https://huggingface.co/datasets/Anthropic/hh-rlhf

## Pre-training

### Data Collection and Preparation

![](../assets/Lesson_4/Ratio_data_source_LLMs.png)

<div align="center">Ratios of various data sources in the pre-training data for existing LLMs (<a href="https://doi.org/10.48550/arXiv.2303.18223">Zhao et al., 2023</a>).</div>

![](../assets/Lesson_4/preprocess_pipeline.png)

<div align="center">An illustration of a typical data preprocessing pipeline for pre-training large language models (<a href="https://doi.org/10.48550/arXiv.2303.18223">Zhao et al., 2023</a>).</div>

- Coding: LLaMA-2 → CodeLLaMA → CodeLLaMA-Python
  - 2T general tokens → 500B code-heavy tokens → 100B Python-heavy tokens

- Mathematics: LLaMA-2 → CodeLLaMA → Llemma
  - 2T general tokens → 500B code-heavy tokens → 50-200B math-heavy tokens

- Long Context: LLaMA-2 → LongLLaMA
  - 1T tokens with 2K context window → 10B tokens with 8K context window

### Architecture

**Model cards of several selected LLMs with public configuration** **details** ([Zhao ](https://doi.org/10.48550/arXiv.2303.18223)[et al., 2023](https://doi.org/10.48550/arXiv.2303.18223))**.**

**(***Here**, PE denotes position embedding, #L denotes the number of layers, #H denotes the number of attention heads,* 

*d**model* *denotes the size of hidden states, and MCL denotes the maximum context length during* *training.)* 

| **Model**  | **Category**    | **Size** | **Normalization** | **PE**   | **Activation** | **Bias** | **#L** | **#H** | **dmodel** | **MCL** |
| ---------- | --------------- | -------- | ----------------- | -------- | -------------- | -------- | ------ | ------ | ---------- | ------- |
| GPT3       | Causal decoder  | 175B     | Pre LayerNorm     | Learned  | GeLU           | ✓        | 96     | 96     | 12288      | 2048    |
| PanGU-α    | Causal decoder  | 207B     | Pre LayerNorm     | Learned  | GeLU           | ✓        | 64     | 128    | 16384      | 1024    |
| OPT        | Causal decoder  | 175B     | Pre LayerNorm     | Learned  | ReLU           | ✓        | 96     | 96     | 12288      | 2048    |
| PaLM       | Causal decoder  | 540B     | Pre LayerNorm     | RoPE     | SwiGLU         | ×        | 118    | 48     | 18432      | 2048    |
| BLOOM      | Causal decoder  | 176B     | Pre LayerNorm     | ALiBi    | GeLU           | ✓        | 70     | 112    | 14336      | 2048    |
| MT-NLG     | Causal decoder  | 530B     | -                 | -        | -              | -        | 105    | 128    | 20480      | 2048    |
| Gopher     | Causal decoder  | 280B     | Pre RMSNorm       | Relative | -              | -        | 80     | 128    | 16384      | 2048    |
| Chinchilla | Causal decoder  | 70B      | Pre RMSNorm       | Relative | -              | -        | 80     | 64     | 8192       | -       |
| Galactica  | Causal decoder  | 120B     | Pre LayerNorm     | Learned  | GeLU           | ×        | 96     | 80     | 10240      | 2048    |
| LaMDA      | Causal decoder  | 137B     | -                 | Relative | GeGLU          | -        | 64     | 128    | 8192       | -       |
| Jurassic-1 | Causal decoder  | 178B     | Pre LayerNorm     | Learned  | GeLU           | ✓        | 76     | 96     | 13824      | 2048    |
| LLaMA      | Causal decoder  | 65B      | Pre RMSNorm       | RoPE     | SwiGLU         | ×        | 80     | 64     | 8192       | 2048    |
| LLaMA 2    | Causal decoder  | 70B      | Pre RMSNorm       | RePE     | SwiGLU         | ×        | 80     | 64     | 8192       | 4096    |
| Falcon     | Causal decoder  | 40B      | Pre LayerNorm     | RoPE     | GeLU           | ×        | 60     | 64     | 8192       | 2048    |
| GLM-130B   | Prefix decoder  | 130B     | Post DeepNorm     | RoPE     | GeGLU          | ✓        | 70     | 96     | 12288      | 2048    |
| T5         | Encoder-decoder | 11B      | Pre RMSNorm       | Relative | ReLU           | ×        | 24     | 128    | 1024       | 512     |

![](../assets/Lesson_4/decoder_block.png)

![multihead_attention](../assets/Lesson_4/multihead_attention.png)

![image-20240701185510335](../assets/Lesson_4/llama2_model_architrecture.png)

**Left: Multi-Head** **Attention** ([Vaswani](https://dl.acm.org/doi/10.5555/3295222.3295349)[ et al., 2017](https://dl.acm.org/doi/10.5555/3295222.3295349))**. Mid: Decoder Block** ([Vaswani](https://dl.acm.org/doi/10.5555/3295222.3295349)[ et al., 2017](https://dl.acm.org/doi/10.5555/3295222.3295349))**.**

**Right: Llama-2-70b-chat-hf tensor file of model structure ( layer 1 ~ layer 78 are omitted)**([Zhao ](https://doi.org/10.48550/arXiv.2303.18223)[et al., 2023](https://doi.org/10.48550/arXiv.2303.18223))**.**

| **Configuration**          | **Method**    | **Equation**                                                 |
| -------------------------- | ------------- | ------------------------------------------------------------ |
| **Normalization Position** | Post Norm     | $$ \text{Norm}(x + \text{Sublayer}(x)) $$                    |
|                            | Pre Norm      | $$ x + \text{Sublayer}(\text{Norm}(x)) $$                    |
|                            | Sandwich Norm | $$ x + \text{Norm}(\text{Sublayer}(\text{Norm}(x))) $$       |
| **Normalization Method**   | LayerNorm     | $$ \frac{x - \mu}{\sigma} \cdot \gamma + \beta, \mu = \frac{1}{d} \sum_{i=1}^{d} x_i, \sigma = \sqrt{\frac{1}{d} \sum_{i=1}^{d} (x_i - \mu)^2} $$ |
|                            | RMSNorm       | $$ \frac{x}{\text{RMS}(x)} \cdot \gamma, \text{RMS}(x) = \sqrt{\frac{1}{d} \sum_{i=1}^{d} x_i^2} $$ |
|                            | DeepNorm      | $$ \text{LayerNorm}(\alpha \cdot x + \text{Sublayer}(x)) $$  |
| **Activation Function**    | ReLU          | $$ \text{ReLU}(x) = \max(x, 0) $$                            |
|                            | GeLU          | $$ \text{GeLU}(x) = 0.5 \cdot x \cdot [1 + \text{erf}(x / \sqrt{2})], \text{erf}(x) = \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} dt $$ |
|                            | Swish         | $$ \text{Swish}(x) = x \cdot \text{sigmoid}(x) $$            |
|                            | SwiGLU        | $$ \text{SwiGLU}(x1, x2) = \text{Swish}(x1) \cdot x2 $$      |
|                            | GeGLU         | $$ \text{GeGLU}(x1, x2) = \text{GeLU}(x1) \cdot x2 $$        |
| **Position Embedding**     | Absolute      | $$ x_i = x_i + p_i $$                                        |
|                            | Relative      | $$ A_{ij} = W_q x_i x_j^T W_k^T + r_{i-j} $$                 |
|                            | RoPE          | $$ A_{ij} = (W_q x_i R_{\Theta, i-j})(W_k x_j R_{\Theta, j})^T $$ |
|                            | ALiBi         | $$ A_{ij} = W_q x_i x_j^T W_k^T - m(i - j) $$                |





### Model Training

| **Model**              | **Batch Size (#tokens)** | **Learning Rate** | **Warmup** | **Decay Method**    | **Optimizer** | **Precision Type** | **Weight Decay** | **Grad Clip** | **Dropout** |
| ---------------------- | ------------------------ | ----------------- | ---------- | ------------------- | ------------- | ------------------ | ---------------- | ------------- | ----------- |
| GPT-3 (175B)           | 32K→3.2M                 | 6 × 10^−5         | yes        | cosine decay to 10% | Adam          | FP16               | 0.1              | 1.0           | -           |
| PanGu-α (200B)         | -                        | 2 × 10^−5         | -          | -                   | Adam          | -                  | 0.1              | -             | -           |
| OPT (175B)             | 2M                       | 1.2 × 10^−4       | yes        | manual decay        | AdamW         | FP16               | 0.1              | -             | 0.1         |
| PaLM (540B)            | 1M→4M                    | 1 × 10^−2         | no         | inverse square root | Adafactor     | BF16               | lr2              | 1.0           | 0.1         |
| BLOOM (176B)           | 4M                       | 6 × 10^−5         | yes        | cosine decay to 10% | Adam          | BF16               | 0.1              | 1.0           | 0.0         |
| MT-NLG (530B)          | 64K→3.75M                | 5 × 10^−5         | yes        | cosine decay to 10% | Adam          | BF16               | 0.1              | 1.0           | -           |
| Gopher (280B)          | 3M→6M                    | 4 × 10^−5         | yes        | cosine decay to 10% | Adam          | BF16               | -                | 1.0           | -           |
| Chinchilla (70B)       | 1.5M→3M                  | 1 × 10^−4         | yes        | cosine decay to 10% | AdamW         | BF16               | -                | -             | -           |
| Galactica (120B)       | 2M                       | 7 × 10^−6         | yes        | linear decay to 10% | AdamW         | -                  | 0.1              | 1.0           | 0.1         |
| LaMDA (137B)           | 256K                     | -                 | -          | -                   | -             | BF16               | -                | -             | -           |
| Jurassic-1 (178B)      | 32K→3.2M                 | 6 × 10^−5         | yes        | -                   | -             | -                  | -                | -             | -           |
| LLaMA (65B)            | 4M                       | 1.5 × 10^−4       | yes        | cosine decay to 10% | AdamW         | -                  | 0.1              | 1.0           | -           |
| LLaMA 2 (70B)          | 4M                       | 1.5 × 10^−4       | yes        | cosine decay to 10% | AdamW         | -                  | 0.1              | 1.0           | -           |
| Falcon (40B)           | 2M                       | 1.85 × 10^−4      | yes        | cosine decay to 10% | AdamW         | BF16               | 0.1              | -             | -           |
| GLM (130B)             | 0.4M→8.25M               | 8 × 10^−5         | yes        | cosine decay to 10% | AdamW         | FP16               | 0.1              | 1.0           | 0.1         |
| T5 (11B)               | 64K                      | 1 × 10^−2         | no         | inverse square root | AdaFactor     | -                  | -                | -             | 0.1         |
| ERNIE 3.0 Titan (260B) | -                        | 1 × 10^−4         | -          | -                   | Adam          | FP16               | 0.1              | 1.0           | -           |
| PanGu-Σ (1.085T)       | 0.5M                     | 2 × 10^−5         | yes        | -                   | Adam          | FP16               | -                | -             | -           |

## References

Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., Joseph, N., Kadavath, S., Kernion, J., Conerly, T., El-Showk, S., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Hume, T., … Kaplan, J. (2022). *Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback* (arXiv:2204.05862). arXiv. http://arxiv.org/abs/2204.05862

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., & Lowe, R. (2022). *Training language models to follow instructions with human feedback* (arXiv:2203.02155). arXiv. https://doi.org/10.48550/arXiv.2203.02155

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research, 21(140), 1–67.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. *Proceedings of the 31st International Conference on Neural Information Processing Systems*, 6000–6010. https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf

Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., … Wen, J.-R. (2023). *A Survey of Large Language Models* (arXiv:2303.18223). arXiv. https://doi.org/10.48550/arXiv.2303.18223
