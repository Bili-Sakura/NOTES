# 【论文导读】大语言模型综述（三）：主流大语言模型介绍

## Info

```markdown
**视频简介** 
本系列为《A Survey of Large Language Model》的论文导读系列视频，本视频导读内容为论文的第三章，即Resources of LLMs部分。
讲演大纲：
- Common Misleadings
	- Model, Agent and Product
	- Who is best
- Publicly Available or Proprietary
- LLaMA Series Model (Meta AI)
- GPT Series Model (OpenAI)
- Awesome Large Language Models
	- GLM Series (Zhipu AI & Tsinghua University)
	- Mixtral Series (Mistral AI)
	- Gemeni Series (Google)
	- Claude Series (Anthropic)
论文引用：
```

## Outline

- [Common Misunderstandings](#common-misunderstandings)
  - Model, Agent and Product
  - Who is best
- [Publicly Available or Proprietary](#publicly-available-or-proprietary)
- [LLaMA Series Model (Meta AI)](#llama-series-model-(meta-ai))
- [GPT Series Model (OpenAI)](#gpt-series-model-(openaI))
- [Awesome Large Language Models](#awesome-large-language-models)
  - [GLM Series (Zhipu AI & Tsinghua University)](#glm-series-(zhipu-ai-&-tsinghua-university))
  - [Mixtral Series (Mistral AI)](#mixstral-series-(mistral-ai))
  - [Gemeni Series (Google)](#gemeni-series-(google))
  - [Claude Series (Anthropic)](#claude-series-(anthropic))

## Common Misunderstandings

### Model, Agent and Product

#### Model

![image-20240621162055308](../assets/Lesson_3/model_illustration.png)

> https://www.youtube.com/watch?v=zjkBMFhNj_g
>
> https://www.jetson-ai-lab.com/tutorial_ollama.html

#### Agent

![image-20240621162329996](../assets/Lesson_3/agent_illustration.png)

> https://chatgpt.com/

#### Product

![image-20240621162410546](../assets/Lesson_3/product_chatgpt.png)

> https://chatgpt.com/

![image-20240621162540577](../assets/Lesson_3/product_chatglm.png)

>  https://chatglm.cn/

#### Relationship

![image-20240621165013639](../assets/Lesson_3/relationship.png)

### Who is best

#### Evalution LLMs on Benchmarks

![image-20240621191852167](../assets/Lesson_3/llms_on_benchmarks.png)

<div align="center">
    Model performance on conventional benchmarks, be it MMLU (<a href="http://arxiv.org/abs/2009.03300">Hendrycks et al., 2021</a>), GPQA (<a href="http://arxiv.org/abs/2311.12022">Rein et al., 2023</a>), MATH (<a href="http://arxiv.org/abs/2103.03874">Hendrycks et al., 2021</a>), HumanEval (<a href="http://arxiv.org/abs/2107.03374">Chen et al., 2021</a>) and so on. A) GPT-4o*. B) LLaMA-3 400B*. C) Claude-3.5 Sonnet*.
</div>
#### Data contamination

![image-20240621193623234](../assets/Lesson_3/data_contamination.png)

<div align="center">
    Left: Contamination Ranking. Right: An overview of detecting approach (<a href="http://arxiv.org/abs/2406.00482">Xu et al., 2024</a>).
</div>
#### Evaluating LLMs by Human Preference

![chaybot_arena_demo1](../assets/Lesson_3/chaybot_arena_demo1.gif)

<div align="center">Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference.*</div>

> https://chat.lmsys.org

![arena_win_rate_matrix_plot](../assets/Lesson_3/arena_win_rate_matrix_plot.png)

![image-20240621203519960](../assets/Lesson_3/leaderboard.png)

<div align="center">
Above:  Fraction of Model A wins for all non-tied A vs. B battles.
Below: Chatbot arena rank leaderboard (<a href="http://arxiv.org/abs/2403.04132">Chiang et al., 2024</a>).*
</div>


## Publicly Available or Proprietary

## LLaMA Series Model (Meta AI)

### Model Files in Huggingface (AI researcher recommended)

![image-20240622102220879](../assets/Lesson_3/model_files_hf.png)

<div align="center">A) LLaMA-2 7B chat model file in Huggingface*. B) Use Transformers to run the model. C) Transparent model architectures and parameters.</div>

> https://huggingface.co/

### Model Files in Ollama (application developer recommended)

![image-20240622102506978](../assets/Lesson_3/model_files_ollama.png)

<div align="center">A) LLaMA-3 8B chat model file in Ollama*. B) Run LLaMA locally using Ollama. C) Ollama application supports mac, linux and windows platforms. D) Storage directory.</div>

> https://ollama.com/

### Evolution of LLaMA

<table>
  <tr>
    <th>Name</th>
    <th>Release date</th>
    <th>Parameters</th>
    <th>Training cost (petaFLOP-day)</th>
    <th>Context length</th>
    <th>Corpus size</th>
    <th>Commercial viability?</th>
  </tr>
  <tr>
    <td rowspan="4">LLaMA (discard) (<a href="http://arxiv.org/abs/2302.13971">Touvron et al., 2023a</a>)</td>
    <td rowspan="4">February 24, 2023</td>
    <td>6.7B</td>
    <td rowspan="4">6,300</td>
    <td rowspan="4">2048</td>
    <td rowspan="4">1–1.4T</td>
    <td rowspan="4" style="background-color: #f8d7da;">No</td>
  </tr>
  <tr>
    <td>13B</td>
  </tr>
  <tr>
    <td>32.5B</td>
  </tr>
  <tr>
    <td>65.2B</td>
  </tr>
  <tr>
    <td rowspan="3">LLaMA 2 (<a href="http://arxiv.org/abs/2307.09288">Touvron et al., 2023b</a>)</td>
    <td rowspan="3">July 18, 2023</td>
    <td>6.7B</td>
    <td rowspan="3">21,000</td>
    <td rowspan="3">4096</td>
    <td rowspan="3">2T</td>
    <td rowspan="3" style="background-color: #d4edda;">Yes</td>
  </tr>
  <tr>
    <td>13B</td>
  </tr>
  <tr>
    <td>69B</td>
  </tr>
  <tr>
    <td rowspan="4">Code LLaMA (<a href="http://arxiv.org/abs/2308.12950">Rozière et al., 2024</a>)</td>
    <td rowspan="4">August 24, 2023</td>
    <td>6.7B</td>
    <td rowspan="4"></td>
    <td rowspan="4">4096</td>
    <td rowspan="4">2T</td>
    <td rowspan="4" style="background-color: #d4edda;">Yes</td>
  </tr>
  <tr>
    <td>13B</td>
  </tr>
  <tr>
    <td>33.7B</td>
  </tr>
  <tr>
    <td>69B</td>
  </tr>
  <tr>
    <td rowspan="4">LLaMA 3</td>
    <td rowspan="4">April 18, 2024</td>
    <td>8B</td>
    <td rowspan="4">100,000</td>
    <td rowspan="4">8192</td>
    <td rowspan="4">15T</td>
    <td rowspan="4" style="background-color: #d4edda;">Yes</td>
  </tr>
  <tr>
    <td>70.6B</td>
  </tr>
  <tr>
    <td>400B+ (unreleased)</td>
  </tr>
</table>

![image-20240622112944760](../assets/Lesson_3/llama2.png)

![Code Llama animation](../assets/Lesson_3/codellama.gif)

![image-20240622113020058](../assets/Lesson_3/llama3.png)

<div align="center">A) LLaMA-2. B) Code LLaMA. C) LLaMA-3.</div>

### Importance of Instruction Tuning

![image-20240622150242878](../assets/Lesson_3/importance_of_it.png)

<div align="center">Given the prompt “1 1 2 3 5 8”, the completetion of different LLaMA models. A) LLaMA-65B (without instruction tuning) (<a href="http://arxiv.org/abs/2302.13971">Touvron et al., 2023a</a>). B) LLaMA-3-8B (without instruction tuning). C) LLaMA-3-8B-chat</div>

## GPT Series Model (OpenAI)

### Access GPT by APIs

![image-20240622150527529](../assets/Lesson_3/access_gpt.png)

<div align="center">Quick start of using OpenAI APIs to access GPT models.* A) Set OPENAI_API_KEY in “.env”. B) Chat with GPT in python. C) Heating models.</div>

> [https://](https://platform.openai.com/docs/models)[platform.openai.com/docs/models](https://platform.openai.com/docs/models)
>
> https://platform.openai.com/docs/quickstart

### Well-developed LLM-based Product

![image-20240622150703797](../assets/Lesson_3/langchain-chatchat.png)

<div align="center">LangChain-Chatchat* Web UI Demo.</div>

> https://github.com/chatchat-space/Langchain-Chatchat/

## Awesome Large Language Models

### GLM Series (Zhipu AI & Tsinghua University)

### Mixtral Series (Mistral AI)

### Gemeni Series (Google)

### Claude Series (Anthropic)

## References

Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. de O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., … Zaremba, W. (2021). *Evaluating Large Language Models Trained on Code* (arXiv:2107.03374). arXiv. http://arxiv.org/abs/2107.03374

Chiang, W.-L., Zheng, L., Sheng, Y., Angelopoulos, A. N., Li, T., Li, D., Zhang, H., Zhu, B., Jordan, M., Gonzalez, J. E., & Stoica, I. (2024). *Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference* (arXiv:2403.04132). arXiv. https://doi.org/10.48550/arXiv.2403.04132

Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., & Steinhardt, J. (2021). *Measuring Massive Multitask Language Understanding* (arXiv:2009.03300). arXiv. https://doi.org/10.48550/arXiv.2009.03300

Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., & Steinhardt, J. (2021). *Measuring Mathematical Problem Solving With the MATH Dataset* (arXiv:2103.03874). arXiv. https://doi.org/10.48550/arXiv.2103.03874

Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., & Bowman, S. R. (2023). *GPQA: A Graduate-Level Google-Proof Q&A Benchmark* (arXiv:2311.12022). arXiv. https://doi.org/10.48550/arXiv.2311.12022

Rozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Sauvestre, R., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori, A., Xiong, W., Défossez, A., … Synnaeve, G. (2024). *Code Llama: Open Foundation Models for Code* (arXiv:2308.12950). arXiv. https://doi.org/10.48550/arXiv.2308.12950

Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., & Lample, G. (2023a). *LLaMA: Open and Efficient Foundation Language Models* (arXiv:2302.13971). arXiv. https://doi.org/10.48550/arXiv.2302.13971

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., … Scialom, T. (2023b). *Llama 2: Open Foundation and Fine-Tuned Chat Models* (arXiv:2307.09288). arXiv. https://doi.org/10.48550/arXiv.2307.09288

Xu, R., Wang, Z., Fan, R.-Z., & Liu, P. (2024). *Benchmarking Benchmark Leakage in Large Language Models* (arXiv:2404.18824). arXiv. https://doi.org/10.48550/arXiv.2404.18824
